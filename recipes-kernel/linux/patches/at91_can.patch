--- a/drivers/net/can/at91_can.c	2017-05-25 23:02:36.253073625 +0200
+++ b/drivers/net/can/at91_can.c	2017-05-26 00:04:50.524356599 +0200
@@ -24,6 +24,7 @@
 #include <linux/if_arp.h>
 #include <linux/interrupt.h>
 #include <linux/kernel.h>
+#include <linux/kfifo.h>
 #include <linux/module.h>
 #include <linux/netdevice.h>
 #include <linux/of.h>
@@ -152,6 +153,18 @@
 	struct at91_can_data *pdata;
 
 	canid_t mb0_id;
+
+/*
+ * The AT91 SoC CAN controller (specially the one in some newer SoCs)
+ * has very little message boxes. On a busy high-speed network, latency
+ * may be too high for napi to catch up before RX overrun occurs.
+ * Therefor we declare a big enough kfifo and fill it directly from
+ * interrupt.
+ */
+
+    #define RX_KFIFO_SIZE 512
+
+    DECLARE_KFIFO_PTR(rx_fifo, struct sk_buff *);
 };
 
 static const struct at91_devtype_data at91_at91sam9263_data = {
@@ -428,7 +441,8 @@
 	priv->can.state = CAN_STATE_ERROR_ACTIVE;
 
 	/* Enable interrupts */
-	reg_ier = get_irq_mb_rx(priv) | AT91_IRQ_ERRP | AT91_IRQ_ERR_FRAME;
+	//reg_ier = get_irq_mb_rx(priv) | AT91_IRQ_ERRP | AT91_IRQ_ERR_FRAME;
+    reg_ier = get_irq_mb_rx(priv) | AT91_IRQ_ERRP;
 	at91_write(priv, AT91_IDR, AT91_IRQ_ALL);
 	at91_write(priv, AT91_IER, reg_ier);
 }
@@ -448,6 +462,28 @@
 	priv->can.state = state;
 }
 
+static int at91_rx_fifo_in(struct net_device *dev, struct sk_buff *skb)
+{
+    struct at91_priv *priv = netdev_priv(dev);
+    unsigned int len = kfifo_put(&priv->rx_fifo, skb);
+
+    if (len)
+        return 0;
+
+    return -ENOMEM;
+}
+
+static int at91_rx_fifo_out(struct net_device *dev, struct sk_buff **skb)
+{
+    struct at91_priv *priv = netdev_priv(dev);
+    unsigned int len = kfifo_get(&priv->rx_fifo, skb);
+
+    if (len)
+        return 0;
+
+    return -ENOENT;
+}
+
 /*
  * theory of operation:
  *
@@ -577,7 +613,8 @@
 
 	cf->can_id |= CAN_ERR_CRTL;
 	cf->data[1] = CAN_ERR_CRTL_RX_OVERFLOW;
-	netif_receive_skb(skb);
+
+    at91_rx_fifo_in(dev, skb);
 
 	stats->rx_packets++;
 	stats->rx_bytes += cf->can_dlc;
@@ -642,7 +679,7 @@
 	}
 
 	at91_read_mb(dev, mb, cf);
-	netif_receive_skb(skb);
+    at91_rx_fifo_in(dev, skb);
 
 	stats->rx_packets++;
 	stats->rx_bytes += cf->can_dlc;
@@ -699,7 +736,7 @@
  * quota.
  *
  */
-static int at91_poll_rx(struct net_device *dev, int quota)
+static int at91_poll_rx(struct net_device *dev)
 {
 	struct at91_priv *priv = netdev_priv(dev);
 	u32 reg_sr = at91_read(priv, AT91_SR);
@@ -707,14 +744,9 @@
 	unsigned int mb;
 	int received = 0;
 
-	if (priv->rx_next > get_mb_rx_low_last(priv) &&
-	    reg_sr & get_mb_rx_low_mask(priv))
-		netdev_info(dev,
-			"order of incoming frames cannot be guaranteed\n");
-
- again:
+again:
 	for (mb = find_next_bit(addr, get_mb_tx_first(priv), priv->rx_next);
-	     mb < get_mb_tx_first(priv) && quota > 0;
+         mb < get_mb_tx_first(priv);
 	     reg_sr = at91_read(priv, AT91_SR),
 	     mb = find_next_bit(addr, get_mb_tx_first(priv), ++priv->rx_next)) {
 		at91_read_msg(dev, mb);
@@ -728,13 +760,14 @@
 			at91_activate_rx_mb(priv, mb);
 
 		received++;
-		quota--;
 	}
 
 	/* upper group completed, look again in lower */
-	if (priv->rx_next > get_mb_rx_low_last(priv) &&
-	    quota > 0 && mb > get_mb_rx_last(priv)) {
+	if ((priv->rx_next > get_mb_rx_low_last(priv)) &&
+        (mb > get_mb_rx_last(priv))) {
+
 		priv->rx_next = get_mb_rx_first(priv);
+
 		goto again;
 	}
 
@@ -789,20 +822,17 @@
 	}
 }
 
-static int at91_poll_err(struct net_device *dev, int quota, u32 reg_sr)
+static int at91_poll_err(struct net_device *dev, u32 reg_sr)
 {
 	struct sk_buff *skb;
 	struct can_frame *cf;
 
-	if (quota == 0)
-		return 0;
-
 	skb = alloc_can_err_skb(dev, &cf);
 	if (unlikely(!skb))
 		return 0;
 
 	at91_poll_err_frame(dev, cf, reg_sr);
-	netif_receive_skb(skb);
+    at91_rx_fifo_in(dev, skb);
 
 	dev->stats.rx_packets++;
 	dev->stats.rx_bytes += cf->can_dlc;
@@ -810,15 +840,14 @@
 	return 1;
 }
 
-static int at91_poll(struct napi_struct *napi, int quota)
+static void at91_poll(struct net_device *dev)
 {
-	struct net_device *dev = napi->dev;
 	const struct at91_priv *priv = netdev_priv(dev);
 	u32 reg_sr = at91_read(priv, AT91_SR);
-	int work_done = 0;
+    u32 reg_ier;
 
 	if (reg_sr & get_irq_mb_rx(priv))
-		work_done += at91_poll_rx(dev, quota - work_done);
+        at91_poll_rx(dev);
 
 	/*
 	 * The error bits are clear on read,
@@ -826,18 +855,31 @@
 	 */
 	reg_sr |= priv->reg_sr;
 	if (reg_sr & AT91_IRQ_ERR_FRAME)
-		work_done += at91_poll_err(dev, quota - work_done, reg_sr);
+        at91_poll_err(dev, reg_sr);
 
-	if (work_done < quota) {
-		/* enable IRQs for frame errors and all mailboxes >= rx_next */
-		u32 reg_ier = AT91_IRQ_ERR_FRAME;
-		reg_ier |= get_irq_mb_rx(priv) & ~AT91_MB_MASK(priv->rx_next);
+    /* enable IRQs for frame errors and all mailboxes >= rx_next */
+    reg_ier = AT91_IRQ_ERR_FRAME;
+    reg_ier |= get_irq_mb_rx(priv) & ~AT91_MB_MASK(priv->rx_next);
+    at91_write(priv, AT91_IER, reg_ier);
+}
 
-		napi_complete(napi);
-		at91_write(priv, AT91_IER, reg_ier);
-	}
+static int at91_napi_poll(struct napi_struct *napi, int quota)
+{
+    struct net_device *dev = napi->dev;
+    const struct at91_priv *priv = netdev_priv(dev);
+    int work_done = 0;
+    struct sk_buff *skb = NULL;
+
+    while(!(kfifo_is_empty(&priv->rx_fifo)) && (work_done < quota)) {
+        at91_rx_fifo_out(dev, &skb);
+        netif_receive_skb(skb);
+        work_done++;
+    }
 
-	return work_done;
+    if(work_done < quota)
+        napi_complete(napi);
+
+    return work_done;
 }
 
 /*
@@ -891,8 +933,8 @@
 	 * we get a TX int for the last can frame directly before a
 	 * wrap around.
 	 */
-	if ((priv->tx_next & get_next_mask(priv)) != 0 ||
-	    (priv->tx_echo & get_next_mask(priv)) == 0)
+	if (((priv->tx_next & get_next_mask(priv)) != 0) ||
+	    ((priv->tx_echo & get_next_mask(priv)) == 0))
 		netif_wake_queue(dev);
 }
 
@@ -1082,7 +1124,6 @@
 {
 	struct net_device *dev = dev_id;
 	struct at91_priv *priv = netdev_priv(dev);
-	irqreturn_t handled = IRQ_NONE;
 	u32 reg_sr, reg_imr;
 
 	reg_sr = at91_read(priv, AT91_SR);
@@ -1091,19 +1132,28 @@
 	/* Ignore masked interrupts */
 	reg_sr &= reg_imr;
 	if (!reg_sr)
-		goto exit;
+		return IRQ_NONE;
 
-	handled = IRQ_HANDLED;
+	/* Receive or error interrupt? -> put in rx_fifo and call napi */
 
-	/* Receive or error interrupt? -> napi */
-	if (reg_sr & (get_irq_mb_rx(priv) | AT91_IRQ_ERR_FRAME)) {
+    //u32 reg_rxe = (get_irq_mb_rx(priv) | AT91_IRQ_ERR_FRAME);
+    u32 reg_rxe = get_irq_mb_rx(priv);
+
+	if (reg_sr & reg_rxe) {
 		/*
 		 * The error bits are clear on read,
 		 * save for later use.
 		 */
 		priv->reg_sr = reg_sr;
-		at91_write(priv, AT91_IDR,
-			   get_irq_mb_rx(priv) | AT91_IRQ_ERR_FRAME);
+		at91_write(priv, AT91_IDR, reg_rxe);
+
+        //at91_poll(dev);
+        at91_poll_rx(dev);
+
+        /* enable IRQs for all mailboxes >= rx_next */
+        reg_rxe &= ~AT91_MB_MASK(priv->rx_next);
+        at91_write(priv, AT91_IER, reg_rxe);
+
 		napi_schedule(&priv->napi);
 	}
 
@@ -1113,8 +1163,7 @@
 
 	at91_irq_err(dev);
 
- exit:
-	return handled;
+	return IRQ_HANDLED;
 }
 
 static int at91_open(struct net_device *dev)
@@ -1355,7 +1404,14 @@
 	priv->pdata = dev_get_platdata(&pdev->dev);
 	priv->mb0_id = 0x7ff;
 
-	netif_napi_add(dev, &priv->napi, at91_poll, get_mb_rx_num(priv));
+    err = kfifo_alloc(&priv->rx_fifo, RX_KFIFO_SIZE, GFP_KERNEL);
+
+    if (err) {
+        dev_err(&pdev->dev, "allocating RX fifo failed\n");
+        goto exit_iounmap;
+    }
+
+    netif_napi_add(dev, &priv->napi, at91_napi_poll, RX_KFIFO_SIZE > 64 ? 64 : RX_KFIFO_SIZE);
 
 	if (at91_is_sam9263(priv))
 		dev->sysfs_groups[0] = &at91_sysfs_attr_group;
